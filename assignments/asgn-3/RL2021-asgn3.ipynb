{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\"> <img style=\"right;\" src=\"logo.png\" width=18% height=18%> Reinforcement Learning | Assignment 3\n",
    "</h1>\n",
    "<br/><br/>\n",
    "\n",
    "This notebook covers the Deep **Critic** approach.\n",
    "\n",
    "Complete the code snippets given in the Section 3: there is a places\n",
    "to insert your code and string fields for your first and last name. The latter are needed to automatically save the results of the algorithms deployment in .json file. After you did that, please upload the notebook (.ipynb) and .json via https://forms.gle/wzqF43ma7yaDuzUc8.\n",
    "* Problem 3.1 - NN Critic (30 points)\n",
    "\n",
    "\n",
    "___Total points:___ 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 1: Theory recap</h2>\n",
    "\n",
    "### Problem\n",
    "\n",
    "Let us reformulate the Pendulum problem from the previous assignment in terms of Actor-Critic.\n",
    "\n",
    "The learning (or, more formally, gradient ascent) here is organized as in the following pseudocode. Generally, the parameters are updated after collecting experience from multiple trajectories of a certain length.\n",
    "\n",
    "The expected reward will be evaluated by a Neural Network (NN), taking concatenated observation and action vectors as input.\n",
    "\n",
    "### Neural Net training\n",
    "\n",
    "The NN is learned in accordance with the assumption that for the perfectly trained model the following equation holds: Q(observation_old, action_old) = reward_curr + gamma * Q(observation_new, action_new). So, with introducing Temporal Difference\n",
    "\n",
    "$TD = reward + \\gamma Q_{new}(x_{new}) - Q_{old}(x_{old})$\n",
    "\n",
    "the loss function for the net could be formulated as\n",
    "\n",
    "$loss = \\frac{1}{2}TD^2$\n",
    "\n",
    "Accordingly, the HJB equation reads like:\n",
    "\n",
    "$Q(obs\\_old, act\\_old) = reward + \\gamma * Q(obs\\_new, act\\_new)$\n",
    "\n",
    "With the necessary modifications to the algorithm of the previous assignment, the pseudocode is\n",
    "\n",
    "<img src=\"nn_critic.png\" alt=\"REINFORCE\" width=75% height=75% />\n",
    "\n",
    "Note the specified \\_new and \\_old subscripts in the pseudocode: they are here to highlight the HJB equation.\n",
    "\n",
    "Another important point is the reinitializing of the environment with the same state on each run. It is a way (one of many ways) of stating the problem. Another way is to sample the initial state of the trajectories in the beginning of the first loop, which is the generalization of the code provided here. One more way is to make a single step after collecting experience from the sampled trajectories, which brings the algorithm closer to MPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 2: NN recap</h2>\n",
    "\n",
    "First, let us recall the process of NN training.\n",
    "\n",
    "This network (which is a single Linear layer) should learn how to multiply its input by 2. The train loop, that starts on 33 line, consists of the following:\n",
    "* data (input and output) generation\n",
    "* zeroing gradients of the model(ltdr for the learning to work correctly)\n",
    "* running the net\n",
    "* applying criterion to judge how far the correct answer is from the model output\n",
    "* calculating gradients of the loss by model parameters\n",
    "* performing the weights modification step\n",
    "\n",
    "If you are not very much familiar with neural networks, examine the code below and familiarize yourself with the methods that are used there. Feel free to play around with the code, uncomment some strings, print random things in order to find out what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "lr = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "#let us print the parameters of the model\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "episodes_num = 200\n",
    "\n",
    "for i in range(episodes_num):\n",
    "    x = torch.tensor([np.random.random_sample()])\n",
    "    y = x * 2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(x)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_val = loss.item()\n",
    "    \n",
    "    if (i % 30 == 0):\n",
    "        print(loss_val)\n",
    "\n",
    "# for param in net.parameters():\n",
    "#     print(param.grad * lr)\n",
    "#     print(param)\n",
    "#     print(\"\")\n",
    "\n",
    "print('Finished Training')\n",
    "print(\"\")\n",
    "print(\"test\")\n",
    "\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "\n",
    "for i in range(5):\n",
    "    x = torch.tensor([np.random.random_sample()])\n",
    "    \n",
    "    outputs = net(x)\n",
    "    \n",
    "    print(x, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\">Section 3: Problems</h2>\n",
    "\n",
    "### <font color=\"blue\">Problem 3.1 - NN Critic </font>\n",
    "\n",
    "Complete the code snippet below: add NN critic into the gradient ascent according to the pseudocode above.\n",
    "\n",
    "* Note the way in which the Pendulum environment is created. It is necessary to manually set the state of the pendulum during the ascent.\n",
    "* Class Q_net is added, feel free to modify it if you need.\n",
    "\n",
    "The output is (as always) a .json file, but in this particular assignment please feel free to modify the length of the training, the length of the episode, to add your custom methods. Generally, the comments ### YOUR SOLUTION BELOW mark those places where the biggest effort is required, but be prepared to modify the code not only there.\n",
    "\n",
    "* You could modify Q_net: widen the hidden layer, add more hidden layers, etc., whatever needed to make it work\n",
    "* Implement custom TD-based loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from gym.envs.classic_control import PendulumEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def parametrized_swing_up_policy(obs, vartheta, s):\n",
    "    #normal random variable\n",
    "    nrv = np.random.normal(0, s, 1)[0]\n",
    "    \n",
    "    if (obs[0] > 0.8):\n",
    "        torque = vartheta[0] * (obs[2] + obs [1]) + nrv\n",
    "        \n",
    "        return [torque], nrv\n",
    "    \n",
    "    else:\n",
    "        return [vartheta[1] * obs[2] + nrv], nrv\n",
    "\n",
    "#x - state\n",
    "#u - action\n",
    "#s - sigma of the normal distribution\n",
    "#nrv - the specific value of the random variable\n",
    "def param_policy_grad(x, u, s, nrv):\n",
    "    if (x[0] > 0.8):\n",
    "        by_0 = nrv * (x[2] + x[1]) / s**2\n",
    "        \n",
    "        return np.array([by_0, 0])\n",
    "\n",
    "    else:\n",
    "        by_1 = nrv * x[2] / s**2\n",
    "        \n",
    "        return np.array([0, by_1])\n",
    "\n",
    "### YOUR SOLUTION BELOW\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self, inp_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inp_dim = inp_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.inp_dim, self.inp_dim)\n",
    "        self.fc2 = nn.Linear(self.inp_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "### YOUR SOLUTION ABOVE\n",
    "\n",
    "ep_len = 340\n",
    "\n",
    "#env = gym.make('Pendulum-v0')\n",
    "env = PendulumEnv()\n",
    "\n",
    "env._max_episode_steps = ep_len\n",
    "\n",
    "def NN_critic(env, update_params, visualize = False):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    ### YOUR SOLUTION BELOW\n",
    "    vartheta = np.array([-10.0, 0.08])\n",
    "\n",
    "    PG_updates_num = 1\n",
    "    episodes_num   = 3\n",
    "    episode_length = 20\n",
    "    \n",
    "    policy = parametrized_swing_up_policy\n",
    "    \n",
    "    alpha = 0.001\n",
    "    sigma = 0.3\n",
    "    gamma = 0.9\n",
    "    ### YOUR SOLUTION ABOVE\n",
    "\n",
    "    reward_history = []\n",
    "        \n",
    "    observation_dim, action_dim = 3, 1\n",
    "    \n",
    "    q_net      = Q_net(observation_dim + action_dim)\n",
    "    q_net_copy = Q_net(observation_dim + action_dim)\n",
    "    \n",
    "    def run_q_net(q_net, observation, action, w):\n",
    "        q_net.load_state_dict(w)\n",
    "                \n",
    "        net_input = torch.tensor([observation[0], observation[1],\n",
    "                                  observation[2], action[0]]).float()\n",
    "        \n",
    "        return q_net(net_input)\n",
    "    \n",
    "    #let us define custom loss function\n",
    "    def TD_loss(q_net, q_net_copy, reward, observation, action,\n",
    "                w_prev, obcervation_curr, action_curr, w):\n",
    "        ### YOUR SOLUTION BELOW\n",
    "        \n",
    "        return loss\n",
    "        ### YOUR SOLUTION ABOVE\n",
    "    \n",
    "    optimizer = optim.SGD(q_net.parameters(), lr=alpha, momentum=0.9)\n",
    "    \n",
    "    start_state = env.state\n",
    "    print(start_state)\n",
    "    \n",
    "    w      = q_net.state_dict()\n",
    "    w_prev = q_net.state_dict()\n",
    "    \n",
    "    for PG_step in range(PG_updates_num):\n",
    "        Grad = np.array([0.0, 0.0])\n",
    "        Sum_Grad_over_episodes = np.array([0.0, 0.0])\n",
    "\n",
    "        sum_param_policy_PDF_grad_acc_over_episodes = 0\n",
    "        \n",
    "        skip_loss_calculation = True\n",
    "        \n",
    "        for ep in range(episodes_num):\n",
    "            #print(\"ep\", ep)\n",
    "            \n",
    "            policy_PDF_grad_acc = np.array([0.0, 0.0])\n",
    "            \n",
    "            ####env.reset_state_into_init_state\n",
    "            env.state = start_state\n",
    "            \n",
    "            for time_step in range(episode_length):\n",
    "                if (visualize == True):\n",
    "                    env.render()\n",
    "                \n",
    "                #print(env.state)\n",
    "                \n",
    "                action, nrv = policy(observation, vartheta, sigma)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                reward_history.append(reward)\n",
    "\n",
    "                #HJB: Q(observation_curr, action_curr) = reward_curr + gamma * Q(observation, action)\n",
    "                \n",
    "                if (not skip_loss_calculation):\n",
    "                    ### YOUR SOLUTION ABOVE\n",
    "\n",
    "                    ### YOUR SOLUTION ABOVE\n",
    "                \n",
    "                else:\n",
    "                    skip_loss_calculation = False\n",
    "                \n",
    "                time.sleep(0.01)\n",
    "                \n",
    "                observation_curr, reward_curr, action_curr = observation, reward, action\n",
    "                \n",
    "                #It's a standard backprop on loss = 1/2 TD^2!\n",
    "                #w -= - alpha_w * TD * grad_Q_NN(observation_curr, action_curr, w)\n",
    "                \n",
    "                #w_prev = w\n",
    "                w_prev = q_net_copy.state_dict()\n",
    "                q_net_copy.load_state_dict(q_net.state_dict())\n",
    "\n",
    "                ppg = param_policy_grad(observation, action, sigma, nrv)\n",
    "                                \n",
    "                Q = run_q_net(q_net, observation, action, w).detach().numpy()\n",
    "                \n",
    "                Grad = ppg * Q\n",
    "\n",
    "                #print(q_net.state_dict())\n",
    "                #print(\"ppg, Q\", ppg, Q)\n",
    "                \n",
    "                Sum_Grad_over_episodes += Grad\n",
    "\n",
    "                sum_param_policy_PDF_grad_acc_over_episodes += policy_PDF_grad_acc\n",
    "\n",
    "        Grad = 1 / episodes_num * Sum_Grad_over_episodes\n",
    "        \n",
    "        #print(\"grad\", Sum_Grad_over_episodes)\n",
    "        \n",
    "        if (update_params):\n",
    "            vartheta += alpha * Grad\n",
    "    \n",
    "    print(vartheta)\n",
    "    \n",
    "    return reward_history\n",
    "\n",
    "nn_critic_reward_history = NN_critic(env, update_params = True, visualize = True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading</font>\n",
    "Run this cell to track your answers and to save your answer for problem 3.1. Make sure you defined the necessary variable above to avoid a `NameError` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADING DO NOT MODIFY\n",
    "from grading_utilities import AnswerTracker\n",
    "asgn3_answers = AnswerTracker()\n",
    "asgn3_answers.record('problem_3-1', {'reward_history': nn_critic_reward_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">Auto-grading: Submit your answers</font>\n",
    "Enter your first and last name in the cell below and then run it to save your answers for this assumption to a JSON file. The file is saved next to this notebook. After the file is created, upload the JSON file and the notebook via the form provided in the beginning of the assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_name = \"asgn_3\"\n",
    "first_name = \"\"\n",
    "last_name = \"\"\n",
    "\n",
    "asgn3_answers.save_to_json(assignment_name, first_name, last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "Reach out to Ilya Osokin (@elijahmipt) on Telegram.\n",
    "\n",
    "## Sources\n",
    "\n",
    "***\n",
    "\n",
    "<sup>[1]</sup> Ng, A. Stanford University, CS229 Notes: Reinforcement Learning and Control.\n",
    "\n",
    "<sup>[2]</sup> Barnabás Póczos, Carnegie Mellon, Introduction To Machine Learning: Reinforcement Learning (Course).\n",
    "\n",
    "<sup>[3]</sup> **Sutton, R. S., Barto, A. G. (2018 ). Reinforcement Learning: An Introduction. The MIT Press.** \n",
    "\n",
    "<sup>[4]</sup> OpenAI: Spinning Up. Retrieved from https://spinningup.openai.com/en/latest/spinningup/rl_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
